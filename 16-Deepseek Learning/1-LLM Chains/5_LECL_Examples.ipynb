{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e394e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3623f59d",
   "metadata": {},
   "source": [
    "### Basic Runnable Sequence: Input → Runnable1 → Runnable2 → ... → Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f4dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "chain = RunnableLambda(lambda x: x + 1) | RunnableLambda(lambda x: x * 2)\n",
    "\n",
    "chain.invoke(5)\n",
    "# Output: 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee9dd1",
   "metadata": {},
   "source": [
    "### Simple LLM Chain: Input → Prompt Template → LLM → Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba8b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Just Return Plain Paragraph withput any newline characters in it.\"),\n",
    "    (\"human\",\"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.8, \n",
    "    top_p=0.95,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"topic\": \"cats\"}).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb16a6f",
   "metadata": {},
   "source": [
    "### With Output Parser: Input → Prompt Template → LLM → Output Parser → Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c28242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Just Return Plain Paragraph withput any newline characters in it.\"),\n",
    "    (\"human\",\"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.8, \n",
    "    top_p=0.95,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"topic\": \"dogs\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa0bd4a",
   "metadata": {},
   "source": [
    "### RAG Retrieval Chain: Input → Retriever → (Context + Question) → Prompt Template → LLM → Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eeda07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace, HuggingFaceEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "text_1 = \"LangChain is great for building LLM apps.\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_texts(texts=[text_1], embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Answer based on context:\\n{context}\\nQuestion: {question}\")\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke(\"What is LangChain and who invented it?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3aed96",
   "metadata": {},
   "source": [
    "### Parallel (RunnableParallel): Input → [Branch1 → Output1, Branch2 → Output2, ...] → Merged Dict → Next → Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc910885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "joke = ChatPromptTemplate.from_template(\"Tell a joke about {topic}\") | model\n",
    "poem = ChatPromptTemplate.from_template(\"Write a 2-line poem about {topic}\") | model\n",
    "\n",
    "parallel = RunnableParallel(joke=joke, poem=poem)\n",
    "\n",
    "parallel.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbedbbf",
   "metadata": {},
   "source": [
    "### With Passthrough: Input → RunnableParallel(Passthrough + Transform) → Prompt Template → LLM → Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774d265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Original: {original}\\nModified: {modified}\")\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "chain = (\n",
    "    RunnableParallel(original=RunnablePassthrough(), modified=lambda x: x.upper())\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "chain.invoke(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafadefb",
   "metadata": {},
   "source": [
    "### Custom Function (RunnableLambda): Input → Custom Function → Next → Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a9d3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def add_ten(x):\n",
    "    return x + 10\n",
    "\n",
    "def multiply_by_two(x):\n",
    "    return x * 2\n",
    "\n",
    "chain = RunnableLambda(add_ten) | RunnableLambda(multiply_by_two)\n",
    "\n",
    "chain.invoke(5)\n",
    "# Output: 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634ce5ef",
   "metadata": {},
   "source": [
    "### Branching and Merging: Input → RunnableParallel(LLM Branch + Other Branch) → Combine Prompt → LLM → Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8412491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "branch1 = RunnableLambda(lambda x: f\"Joke: {x}\")\n",
    "branch2 = RunnableLambda(lambda x: f\"Poem: {x}\")\n",
    "\n",
    "parallel = RunnableParallel(joke=branch1, poem=branch2)\n",
    "\n",
    "combine_prompt = ChatPromptTemplate.from_template(\"{joke}\\n{poem}\\nSummarize both:\")\n",
    "chain = parallel | combine_prompt | model\n",
    "\n",
    "chain.invoke(\"cats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08191a0b",
   "metadata": {},
   "source": [
    "### Itemgetter Routing: Input Dict → Itemgetter(key) → Runnable → Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a26d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "chain = (\n",
    "    {\"topic\": itemgetter(\"input\")}\n",
    "    | RunnableLambda(lambda d: d[\"topic\"].upper())\n",
    ")\n",
    "\n",
    "chain.invoke({\"input\": \"langchain\"})\n",
    "# Output: 'LANGCHAIN'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9ad4a8",
   "metadata": {},
   "source": [
    "### Conditional (via routing): Input → Router Runnable → Selected Chain → Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7bc1fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\GenAI - CampusX\\gen_ai_campusx\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Certainly! Let's solve the problem step-by-step.\\n\\n1. **Identify the Numbers and Operation**: The expression given is 2 + 2. Here, we have two numbers (2 and 2) and an addition operation (+).\\n\\n2. **Understand the Addition Operation**: Addition is an operation that combines the quantities of two or more numbers. When we add two numbers, we are essentially counting how many items we have in total if we put both groups together.\\n\\n3. **Perform the Addition**: \\n   - Start with the first number: 2\\n   - Add the second number: 2 more\\n\\n4. **Count the Total**: \\n   - If you have 2 items and you add 2 more items, you end up with 4 items in total.\\n\\n5. **Conclusion**: The result of 2 + 2 is 4.\\n\\nSo, the final answer is **4**.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 188, 'prompt_tokens': 49, 'total_tokens': 237}, 'model_name': 'Qwen/Qwen2.5-7B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019b8c47-d66e-7b82-b2c3-deaf0941154b-0' usage_metadata={'input_tokens': 49, 'output_tokens': 188, 'total_tokens': 237}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableBranch, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "general_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"Answer the following question generally: {question}\")\n",
    "])\n",
    "\n",
    "math_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"You are a math expert. Solve the following step-by-step: {question}\")\n",
    "])\n",
    "\n",
    "general = general_prompt | model\n",
    "math = math_prompt | model\n",
    "\n",
    "# Extract the question string first, then branch on the string\n",
    "extract_question = RunnableLambda(lambda input_dict: input_dict[\"question\"])\n",
    "\n",
    "branch = RunnableBranch(\n",
    "    (lambda q: \"math\" in q.lower() or any(op in q.lower() for op in [\"+\", \"-\", \"*\", \"/\", \"calculate\", \"solve\", \"=\"]), math),\n",
    "    general\n",
    ")\n",
    "\n",
    "chain = extract_question | branch\n",
    "\n",
    "# Test\n",
    "print(chain.invoke({\"question\": \"What is 2+2?\"}))\n",
    "\n",
    "# print(chain.invoke({\"question\": \"Tell me about quantum physics\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2a7e04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai_campusx (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
