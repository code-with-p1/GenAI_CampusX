{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85f90e36",
   "metadata": {},
   "source": [
    "##### TransformChain - A chain that applies a Python function (not LLM) to transform input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8393642c",
   "metadata": {},
   "source": [
    "##### Input → Custom Function → Transformed Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6726c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d1082a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict, Any, TypedDict\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51b9537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.8, \n",
    "    top_p=0.95,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f22c7f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────\n",
    "# Modern style: using RunnableLambda + @chain decorator\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "# Option 1: Clean input (most readable style today)\n",
    "# @chain\n",
    "def clean_user_input(inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Clean and preprocess user input\"\"\"\n",
    "    text = inputs[\"user_input\"].strip().lower()\n",
    "    text = \" \".join(text.split())  # normalize whitespace\n",
    "    \n",
    "    words = text.split()\n",
    "    keywords = [w for w in words if len(w) > 3][:5]\n",
    "    \n",
    "    return {\n",
    "        \"cleaned_input\": text,\n",
    "        \"word_count\": len(words),\n",
    "        \"potential_keywords\": \", \".join(keywords),\n",
    "        \"original_input\": text,           # sometimes useful to keep\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0824d6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────\n",
    "# Analysis prompt (modern style — often use ChatPromptTemplate)\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "analysis_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a concise intent & sentiment analyzer.\n",
    "        Always respond strictly in this format:\n",
    "\n",
    "        1. Sentiment (positive/negative/neutral)\n",
    "        2. Main Intent (question/complaint/request/feedback/other)\n",
    "        3. Urgency Level (low/medium/high)\n",
    "        4. Suggested Response Category (support/sales/returns/technical/other)\"\"\"),\n",
    "    (\"user\", \"\"\"Text: {cleaned_input}\n",
    "        Word count: {word_count}\n",
    "        Keywords: {potential_keywords}\"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36f152ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common modern composition style\n",
    "analysis_chain = (analysis_prompt | chat_model | StrOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01a80f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────\n",
    "# Combined pipeline (most popular patterns in 2024/2025)\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "# Style A — clean & recommended (RunnableLambda + pipe)\n",
    "clean_and_analyze = (\n",
    "    RunnableLambda(clean_user_input)\n",
    "    | analysis_chain\n",
    ")\n",
    "\n",
    "# Style B — even more compact with @chain\n",
    "@chain\n",
    "def clean_and_analyze_compact(user_input: str) -> str:\n",
    "    cleaned = clean_user_input.invoke({\"user_input\": user_input})\n",
    "    return analysis_chain.invoke(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c831855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================================\n",
      "MODERN LANGCHAIN PIPELINE (2025 style)\n",
      "=================================================================\n",
      "\n",
      "── Test 1 ──\n",
      "Input : \" HELLO, I need HELP with my ORDER! It's been 5 DAYS!!! \"\n",
      "\n",
      "Analysis:\n",
      "1. Sentiment: neutral\n",
      "2. Main Intent: request\n",
      "3. Urgency Level: high\n",
      "4. Suggested Response Category: support\n",
      "\n",
      "── Test 2 ──\n",
      "Input : 'Can you please tell me about return policy? Thanks.'\n",
      "\n",
      "Analysis:\n",
      "1. Positive\n",
      "2. Request\n",
      "3. Low\n",
      "4. Returns\n",
      "\n",
      "── Test 3 ──\n",
      "Input : 'This product is terrible. Worst experience ever. DO NOT BUY!'\n",
      "\n",
      "Analysis:\n",
      "1. Sentiment (negative)\n",
      "2. Main Intent (complaint)\n",
      "3. Urgency Level (high)\n",
      "4. Suggested Response Category (returns)\n"
     ]
    }
   ],
   "source": [
    "# ────────────────────────────────────────────────\n",
    "# Usage examples\n",
    "# ────────────────────────────────────────────────\n",
    "test_inputs = [\n",
    "    \" HELLO, I need HELP with my ORDER! It's been 5 DAYS!!! \",\n",
    "    \"Can you please tell me about return policy? Thanks.\",\n",
    "    \"This product is terrible. Worst experience ever. DO NOT BUY!\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*65)\n",
    "print(\"MODERN LANGCHAIN PIPELINE (2025 style)\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "for i, text in enumerate(test_inputs, 1):\n",
    "    print(f\"\\n── Test {i} ──\")\n",
    "    print(f\"Input : {text!r}\")\n",
    "    \n",
    "    # Option A\n",
    "    result = clean_and_analyze.invoke({\"user_input\": text})\n",
    "    print(\"\\nAnalysis:\")\n",
    "    print(result)\n",
    "    \n",
    "    # Option B (more direct)\n",
    "    # result = clean_and_analyze_compact.invoke(text)\n",
    "    # print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eac03cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Replace with your actual LLM\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "llm = chat_model\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 1. Clean user input\n",
    "# ────────────────────────────────────────────────\n",
    "@chain\n",
    "def clean_user_input(inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    text = inputs[\"user_input\"].strip().lower()\n",
    "    text = \" \".join(text.split())\n",
    "    words = text.split()\n",
    "    keywords = [w for w in words if len(w) > 3][:5]\n",
    "    \n",
    "    return {\n",
    "        \"cleaned_input\": text,\n",
    "        \"word_count\": len(words),\n",
    "        \"potential_keywords\": \", \".join(keywords),\n",
    "        \"original_query\": inputs[\"user_input\"]\n",
    "    }\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 2. Format retrieved documents (formerly format_api_response)\n",
    "# ────────────────────────────────────────────────\n",
    "@chain\n",
    "def format_retrieved_docs(inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Expecting: inputs[\"retrieved_docs\"] = list of dicts or strings\n",
    "    Formats them into clean, readable context for the LLM\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\\n\\n\", inputs, \"\\n\\n\\n\")\n",
    "    \n",
    "    raw_docs = inputs[\"retrieved_docs\"]\n",
    "    \n",
    "    if not raw_docs:\n",
    "        formatted_context = \"No relevant information retrieved.\"\n",
    "    else:\n",
    "        formatted_parts = []\n",
    "        for i, doc in enumerate(raw_docs[:5], 1):  # Limit to top 5\n",
    "            if isinstance(doc, dict):\n",
    "                content = doc.get(\"content\") or doc.get(\"text\") or str(doc)\n",
    "                source = doc.get(\"source\", f\"document_{i}\")\n",
    "                score = doc.get(\"score\", doc.get(\"relevance_score\", \"N/A\"))\n",
    "            else:\n",
    "                content = str(doc)\n",
    "                source = f\"document_{i}\"\n",
    "                score = \"N/A\"\n",
    "            \n",
    "            preview = content[:600] + \"...\" if len(content) > 600 else content\n",
    "            part = f\"\"\"[Document {i}]\n",
    "Source: {source}\n",
    "Relevance: {score}\n",
    "\n",
    "{preview}\n",
    "\n",
    "\"\"\"\n",
    "            formatted_parts.append(part)\n",
    "        \n",
    "        formatted_context = \"\\n\".join(formatted_parts)\n",
    "    \n",
    "    return {\n",
    "        \"context\": formatted_context,\n",
    "        \"doc_count\": len(raw_docs),\n",
    "        \"sources_used\": \", \".join(\n",
    "            [d.get(\"source\", f\"doc_{i}\") for i, d in enumerate(raw_docs[:5], 1)]\n",
    "            if raw_docs else [\"none\"]\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 3. Final RAG + Analysis Chain\n",
    "# ────────────────────────────────────────────────\n",
    "rag_analysis_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert customer support analyst.\n",
    "Use the provided context (retrieved knowledge) and user query to give a precise, helpful response.\n",
    "\n",
    "Guidelines:\n",
    "- Base your answer primarily on the retrieved context if relevant.\n",
    "- If context is irrelevant or empty, say so clearly.\n",
    "- Always include: sentiment, intent, urgency, and suggested action.\n",
    "- Be professional and concise.\n",
    "\"\"\"),\n",
    "    (\"human\", \"\"\"User Query: {cleaned_input}\n",
    "Original: {original_query}\n",
    "   \n",
    "Analysis required:\n",
    "1. Sentiment (positive/negative/neutral)\n",
    "2. Main Intent (question/complaint/request/feedback/other)\n",
    "3. Urgency Level (low/medium/high)\n",
    "4. Suggested Response Category (support/returns/billing/technical/sales/other)\n",
    "5. Brief Recommended Reply (2-3 sentences max)\n",
    "\n",
    "If no useful context, note: \"No relevant knowledge found.\" \"\"\")\n",
    "])\n",
    "\n",
    "final_chain = (\n",
    "    # Step 1: Clean input\n",
    "    clean_user_input\n",
    "    # Step 2: Simulate retrieval (replace this lambda with real retriever)\n",
    "    | RunnableLambda(lambda x: {\n",
    "        **x,\n",
    "        # MOCK RETRIEVAL — replace with actual vectorstore.as_retriever().invoke(...)\n",
    "        \"retrieved_docs\": [\n",
    "            {\"content\": \"Our return policy allows returns within 30 days of purchase with original receipt. Refunds are processed within 5-7 business days.\", \"source\": \"returns_policy.md\", \"score\": 0.92},\n",
    "            {\"content\": \"Orders typically ship within 1-2 business days. Delays may occur during holidays.\", \"source\": \"shipping_faq.html\", \"score\": 0.85},\n",
    "        ] if any(kw in x[\"cleaned_input\"] for kw in [\"return\", \"order\", \"ship\", \"delay\"]) else []\n",
    "    })\n",
    "    # Step 3: Format retrieved docs\n",
    "    | format_retrieved_docs\n",
    "    # Step 4: Generate final analysis\n",
    "    | rag_analysis_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# Test the full pipeline\n",
    "# ────────────────────────────────────────────────\n",
    "test_inputs = [\n",
    "    \"HELLO, I need HELP with my ORDER! It's been 5 DAYS!!! \",\n",
    "    \"Can you please tell me about return policy? Thanks.\",\n",
    "    \"This product is terrible. Worst experience ever. DO NOT BUY!\",\n",
    "    \"What are your opening hours on weekends?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FULL RAG + FORMATTING + ANALYSIS PIPELINE (2025 LangChain Style)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    for i, user_input in enumerate(test_inputs, 1):\n",
    "        print(f\"\\n{'─'*30} TEST CASE {i} {'─'*30}\")\n",
    "        print(f\"Input: {user_input!r}\")\n",
    "        \n",
    "        result = final_chain.invoke({\"user_input\": user_input})\n",
    "        \n",
    "        print(\"\\nFinal Analysis:\")\n",
    "        print(result)\n",
    "        print(\"─\" * 70)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcf879b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80634969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai_campusx (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
